{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpHL0BKxj-J_"
      },
      "outputs": [],
      "source": [
        "pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "6FlstD_tZrwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#torch.manual_seed(42)\n",
        "from IPython.display import clear_output\n",
        "torch_version = torch.__version__\n",
        "print(\"Torch version: \", torch_version)\n",
        "pytorch_version = f\"torch-{torch.__version__}.html\"\n",
        "!pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "#!pip install torch-geometric\n",
        "clear_output()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "NuVpfLbQZu6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version"
      ],
      "metadata": {
        "id": "N28_3hjjMxfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "Qh7RULSlNlGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch_geometric.__version__)"
      ],
      "metadata": {
        "id": "YhHXuECjNXkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ynq-wsUjBrH"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "from torch_geometric.nn import Explainer, PGExplainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsNplr68poFz"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#from torch_geometric.data import Data\n",
        "#from torch_geometric.nn import GINConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vw_7f-Nruf0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch_geometric.nn as gnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WiTiKNivvO7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K5RyD1XwI-4"
      },
      "outputs": [],
      "source": [
        "!unzip gdrive/My\\ Drive/GNN_data/pt_2_files.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cHEfKebP-Z-"
      },
      "outputs": [],
      "source": [
        "!unzip gdrive/My\\ Drive/GNN_data/pt_1_files.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC4EEHIcwp7E"
      },
      "outputs": [],
      "source": [
        "!unzip gdrive/My\\ Drive/GNN_data/pt_files.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqhprypxlLj7"
      },
      "outputs": [],
      "source": [
        "# Set the directory where the '.pt' files are stored\n",
        "data_dir = \"pt_files\"\n",
        "data_dir_1 = \"pt_1_files\"\n",
        "data_dir_2 = \"pt_2_files\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm9p-bBzkDHa"
      },
      "outputs": [],
      "source": [
        "pt_files = os.listdir(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZlus0ERQGwz"
      },
      "outputs": [],
      "source": [
        "pt_1_files = os.listdir(data_dir_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkxbTxqJxHJ1"
      },
      "outputs": [],
      "source": [
        "pt_2_files = os.listdir(data_dir_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woeiTSYWxm9T"
      },
      "outputs": [],
      "source": [
        "# Split the list of '.pt' files into training, validation, and test sets\n",
        "n_train = int(len(pt_files) * 0.8)\n",
        "n_val = int(len(pt_files) * 0.1)\n",
        "train_pt_files, val_pt_files, test_pt_files = pt_files[:n_train], pt_files[n_train:n_train+n_val], pt_files[n_train+n_val:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVgnhXkAarQv"
      },
      "outputs": [],
      "source": [
        "# Split the list of '.pt' files into training, validation, and test sets\n",
        "n_train_1 = int(len(pt_1_files) * 0.8)\n",
        "n_val_1 = int(len(pt_1_files) * 0.1)\n",
        "train_pt_1_files, val_pt_1_files, test_pt_1_files = pt_1_files[:n_train], pt_1_files[n_train:n_train+n_val], pt_1_files[n_train+n_val:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLfiNTcTx0U7"
      },
      "outputs": [],
      "source": [
        "# Split the list of '_2.pt' files into training, validation, and test sets\n",
        "n_train_2 = int(len(pt_2_files) * 0.8)\n",
        "n_val_2 = int(len(pt_2_files) * 0.1)\n",
        "train_pt_2_files, val_pt_2_files, test_pt_2_files = pt_2_files[:n_train_2], pt_2_files[n_train_2:n_train_2+n_val_2], pt_2_files[n_train_2+n_val_2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R5xYYoAyNz8"
      },
      "outputs": [],
      "source": [
        "# Combine the training data sets from '.pt' and '_2.pt' files\n",
        "train_data_files = train_pt_files + train_pt_1_files + train_pt_2_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHJfQAkhEx26"
      },
      "outputs": [],
      "source": [
        "val_data_files = val_pt_files + val_pt_1_files + val_pt_2_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WXs-o65Ex_i"
      },
      "outputs": [],
      "source": [
        "test_data_files = test_pt_files + test_pt_1_files + test_pt_2_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYFj7RCmFGwB"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_paths):\n",
        "        self.data = [torch.load(file_path) for file_path in file_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "_Kgl2KVuunYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = []\n",
        "for file_name in train_data_files:\n",
        "    file_path = os.path.join(data_dir, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        file_path = os.path.join(data_dir_1, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        file_path = os.path.join(data_dir_2, file_name)\n",
        "    if os.path.exists(file_path):\n",
        "        file_paths.append(file_path)\n",
        "\n",
        "\n",
        "# Create an instance of your dataset with the full paths\n",
        "dataset_train = MyDataset(file_paths)\n",
        "\n",
        "# Create a DataLoader from the dataset\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "rD4yY22jK6h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(val_dataloader))\n",
        "print(f\"x shape: {data.x.shape}\")\n",
        "print(f\"edge_index shape: {data.edge_index.shape}\")\n",
        "print(f\"y shape: {data.y.shape}\")\n"
      ],
      "metadata": {
        "id": "GY-jp2P_AT7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlVRh5lrF4sj"
      },
      "outputs": [],
      "source": [
        "file_paths = []\n",
        "for file_name in val_data_files:\n",
        "    file_path = os.path.join(data_dir, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        file_path = os.path.join(data_dir_1, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        file_path = os.path.join(data_dir_2, file_name)\n",
        "    if os.path.exists(file_path):\n",
        "        file_paths.append(file_path)\n",
        "\n",
        "\n",
        "# Create an instance of your dataset with the full paths\n",
        "dataset_val = MyDataset(file_paths)\n",
        "\n",
        "# Create a DataLoader from the dataset\n",
        "val_dataloader = DataLoader(dataset_val, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL7MqICuGGyq"
      },
      "outputs": [],
      "source": [
        "file_paths = []\n",
        "for file_name in test_data_files:\n",
        "    file_path = os.path.join(data_dir, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        file_path = os.path.join(data_dir_1, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        file_path = os.path.join(data_dir_2, file_name)\n",
        "    if os.path.exists(file_path):\n",
        "        file_paths.append(file_path)\n",
        "\n",
        "\n",
        "# Create an instance of your dataset with the full paths\n",
        "dataset_test = MyDataset(file_paths)\n",
        "\n",
        "# Create a DataLoader from the dataset\n",
        "test_dataloader = DataLoader(dataset_test, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l_2zQbuGYFD"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVAIP1LyGu5q"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "        self.conv1 = GCNConv(input_dim, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 3)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
        "        print(edge_index.shape)\n",
        "        print(edge_index)\n",
        "        x = self.conv1(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index, edge_weight)\n",
        "        x = gnn.global_mean_pool(x, batch)\n",
        "        x = self.lin(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "# Define criterion and optimizer\n",
        "model = GCN(input_dim = 3, hidden_channels = 256)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "model = model.to(device)\n",
        "model = model.double()\n",
        "# Train and evaluate the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for data in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x.to(device), data.edge_index.to(device).long(), data.batch.to(device))\n",
        "        loss = criterion(out, data.y.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y.to(device)).sum())\n",
        "        #print(correct)\n",
        "    train_acc = correct / len(train_dataloader.dataset)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in val_dataloader:\n",
        "        out = model(data.x.to(device), data.edge_index.to(device).long(), data.batch.to(device))\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y.to(device)).sum())\n",
        "    val_acc = correct / len(val_dataloader.dataset)\n",
        "    print(f'Epoch: {epoch}, Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(input_dim = 3, hidden_channels = 256)\n",
        "#model.load_state_dict(torch.load('model1.pth'))\n",
        "model.to(device)\n",
        "#model.load_state_dict(torch.load('model1.pth'))\n",
        "#model.eval()"
      ],
      "metadata": {
        "id": "cv7I3FU793Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('model1.pth', map_location=device))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "I9NE903vITX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.explain import Explainer, PGExplainer\n",
        "explainer = Explainer(\n",
        "    model=model,\n",
        "    algorithm=PGExplainer(epochs=10, lr=0.003).to(device),\n",
        "    explanation_type='phenomenon',\n",
        "    edge_mask_type='object',\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='graph',\n",
        "        return_type='raw',\n",
        "    ),\n",
        "    # Include only the top 10 most important edges:\n",
        "    threshold_config=dict(threshold_type='topk', value=5),\n",
        ")\n",
        "# PGExplainer needs to be trained separately since it is a parametric\n",
        "# explainer i.e it uses a neural network to generate explanations:\n",
        "for epoch in range(10):\n",
        "    for data in test_dataloader:\n",
        "        # Move the data to the chosen device\n",
        "        data = data.to(device)\n",
        "        x = data.x\n",
        "        x = x.to(device)\n",
        "\n",
        "        edge_index = data.edge_index\n",
        "        edge_index = edge_index.to(device)\n",
        "\n",
        "        target = data.y\n",
        "        target = target.to(device)\n",
        "\n",
        "        batch = data.batch\n",
        "        batch = batch.to(device)\n",
        "\n",
        "\n",
        "        # Pass the data to the model's train method\n",
        "        loss = explainer.algorithm.train(epoch, model, x, edge_index, target=target, batch=batch)\n",
        "# Generate the explanation for a particular graph:\n",
        "explanation = explainer(dataset[0].x, dataset[0].edge_index)\n",
        "print(explanation.edge_mask)"
      ],
      "metadata": {
        "id": "TBGRVnPXwb2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install captum"
      ],
      "metadata": {
        "id": "KDa86gxJQfJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_first_graph(dataloader):\n",
        "    for batch in dataloader:\n",
        "        graph = batch[0]\n",
        "        return graph"
      ],
      "metadata": {
        "id": "YFGRNvSRv3J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = get_first_graph(test_dataloader)"
      ],
      "metadata": {
        "id": "jswFzOVfv6Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = graph.y\n",
        "print(y)"
      ],
      "metadata": {
        "id": "dcADW9IebrUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_second_graph(dataloader):\n",
        "    count = 0\n",
        "    for batch in dataloader:\n",
        "        count += 1\n",
        "        if count == 9:\n",
        "            graph = batch[0]\n",
        "            return graph\n"
      ],
      "metadata": {
        "id": "tMFXUP3wcAFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph2 = get_second_graph(test_dataloader)\n",
        "y = graph2.y\n",
        "print(y)"
      ],
      "metadata": {
        "id": "yz704xNHcst9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import Saliency, IntegratedGradients\n",
        "\n",
        "def model_forward(edge_mask, data):\n",
        "    batch = torch.zeros(data.x.shape[0], dtype=int).to(device)\n",
        "    out = model(data.x, data.edge_index, batch, edge_mask)\n",
        "    return out\n",
        "\n",
        "\n",
        "def explain(method, data, target=0):\n",
        "    input_mask = torch.ones(data.edge_index.shape[1]).requires_grad_(True).to(device)\n",
        "    if method == 'ig':\n",
        "        ig = IntegratedGradients(model_forward)\n",
        "        mask = ig.attribute(input_mask, target=target,\n",
        "                            additional_forward_args=(data,),\n",
        "                            internal_batch_size=data.edge_index.shape[1])\n",
        "    elif method == 'saliency':\n",
        "        saliency = Saliency(model_forward)\n",
        "        mask = saliency.attribute(input_mask, target=target,\n",
        "                                  additional_forward_args=(data,))\n",
        "    else:\n",
        "        raise Exception('Unknown explanation method')\n",
        "\n",
        "    edge_mask = np.abs(mask.cpu().detach().numpy())\n",
        "    if edge_mask.max() > 0:  # avoid division by zero\n",
        "        edge_mask = edge_mask / edge_mask.max()\n",
        "    return edge_mask"
      ],
      "metadata": {
        "id": "RlA_Y8cjhQaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def aggregate_edge_directions(edge_mask, data):\n",
        "    edge_mask_dict = defaultdict(float)\n",
        "    for val, u, v in list(zip(edge_mask, *data.edge_index)):\n",
        "        u, v = u.item(), v.item()\n",
        "        if u > v:\n",
        "            u, v = v, u\n",
        "        edge_mask_dict[(u, v)] += val\n",
        "    return edge_mask_dict\n",
        "\n",
        "\n",
        "for title, method in [('Integrated Gradients', 'ig'), ('Saliency', 'saliency')]:\n",
        "    edge_mask = explain(method, graph.to(device), target=0)\n",
        "    edge_mask_dict = aggregate_edge_directions(edge_mask, graph.to(device))"
      ],
      "metadata": {
        "id": "nVSV2Ck91pBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for title, method in [('Integrated Gradients', 'ig'), ('Saliency', 'saliency')]:\n",
        "    edge_mask = explain(method, graph2.to(device), target=0)\n",
        "    edge_mask_dict = aggregate_edge_directions(edge_mask, graph2.to(device))\n",
        "    print(f\"Method: {title}\")\n",
        "    print(\"Edge\\tScore\")\n",
        "    for edge, score in sorted(edge_mask_dict.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
        "        print(f\"{edge}\\t{score}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "adc9jm5XTdcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}